{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "Hides code for easier viewing, Toggle code boxes <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code adapted from \"PHAS0007 Script for session 5: Notebook 1 (of 2)\",(Dash and Lemos, 2019) {3}\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "Hides code for easier viewing, Toggle code boxes <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neutrino event classification\n",
    "\n",
    "This mini-project's dataset is comprised of a number of small files containing images of simulated neutrino interactions in a hypothetical detector that looks an awful lot like the detectors of the NOvA experiment. For each neutrino interaction the images consist of two $100 \\times 80$ pixel images that represent the $x \\times z$ and $y \\times z$ projections of the tracks of particles in the detector.\n",
    "\n",
    "The data for this mini-project comes in the form of the following files:\n",
    "\n",
    "| File | Description |\n",
    "| ----------- | ----------- |\n",
    "| neutrino1.h5 | The 1st HDF5 file containing event images and meta deta |\n",
    "| $\\vdots$ | The middle ones |\n",
    "| neutrino200.h5| The 200th HDF5 file|\n",
    "\n",
    "\n",
    "The images show the energy deposited by simulated neutrinos in a NOvA like detector. Some of the meta information in the hdf5 file is described below\n",
    "\n",
    "| Label | Description |\n",
    "| ----------- | ----------- |\n",
    "| neutrino/nuenergy | Neutrino Energy (GeV) |\n",
    "| neutrino/lepenergy | Lepton Energy (GeV) |\n",
    "| neutrino/finalstate | Interaction |\n",
    "| neutrino/finalstate | Final State |\n",
    " \n",
    "\n",
    "The [PDG code](https://pdg.lbl.gov/2019/reviews/rpp2019-rev-monte-carlo-numbering.pdf) is a number which identifies the particle type (e.g electron=11, electron-neutrino=12, etc.)\n",
    "\n",
    "The $interaction$ says what kind of interaction occured and is defined in the enumeration below.\n",
    "\n",
    "## Machine learning tasks\n",
    "1. Develop a machine learning classifier that can successfully identify $\\nu_\\mu$ charged-current events\n",
    "2. Test your machine learning classifier and investigate how the efficiency of the classifier depends on the meta data variables shown above\n",
    "\n",
    "### Potential extensions\n",
    "1. Write a machine learning algorithm to determine the energy of the neutrino\n",
    "2. Write a machine learning algorithm to determine the flavour of the neutrino\n",
    "3. Write a machine learning algorithm to determine $y=$ lepton energy over neutrino energy\n",
    "4. Write a machine learning algorithm to determine the number of protons or pions\n",
    "5. Write a machine learning algorithm to determine the interaction mode.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import for processing data files\n",
    "import h5py\n",
    "\n",
    "# Standard imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# Added for a progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TensorFlow and tf.keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "# Import for analysising model predicted results\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes encoding the interactions and states\n",
    "\n",
    "import enum \n",
    "class Interaction(enum.Enum):\n",
    "    kNumuQE =0           # Numu CC QE interaction\n",
    "    kNumuRes =1           # Numu CC Resonant interaction\n",
    "    kNumuDIS = 2          # Numu CC DIS interaction\n",
    "    kNumuOther = 3        # Numu CC, other than above\n",
    "    kNueQE = 4            # Nue CC QE interaction\n",
    "    kNueRes = 5           # Nue CC Resonant interaction\n",
    "    kNueDIS = 6           # Nue CC DIS interaction\n",
    "    kNueOther = 7         # Nue CC, other than above\n",
    "    kNutauQE = 8          # Nutau CC QE interaction\n",
    "    kNutauRes = 9         # Nutau CC Resonant interaction\n",
    "    kNutauDIS =10         # Nutau CC DIS interaction\n",
    "    kNutauOther =11       # Nutau CC, other than above\n",
    "    kNuElectronElastic = 12# NC Nu On E Scattering\n",
    "    kNC =13                # NC interaction\n",
    "    kCosmic =14           # Cosmic ray background\n",
    "    kOther =15            # Something else.  Tau?  Hopefully we don't use this\n",
    "    kNIntType=16          # Number of interaction types, used like a vector size\n",
    "\n",
    "    \n",
    "class FinalState(enum.Enum):\n",
    "    kNumu0tr0sh=0          # Numu CC - no track no shower\n",
    "    kNumu0tr1sh=1          # Numu CC - no track  1 shower\n",
    "    kNumu0tr2sh=enum.auto()          # Numu CC - no track  2 shower\n",
    "    kNumu0trMsh=enum.auto()          # Numu CC - no track 3+ shower\n",
    "    kNumu1tr0sh=enum.auto()          # Numu CC -  1 track no shower\n",
    "    kNumu1tr1sh=enum.auto()          # Numu CC -  1 track  1 shower\n",
    "    kNumu1tr2sh=enum.auto()          # Numu CC -  1 track  2 shower\n",
    "    kNumu1trMsh=enum.auto()          # Numu CC -  1 track 3+ shower\n",
    "    kNumu2tr0sh=enum.auto()          # Numu CC -  2 track no shower\n",
    "    kNumu2tr1sh=enum.auto()          # Numu CC -  2 track  1 shower\n",
    "    kNumu2tr2sh=enum.auto()          # Numu CC -  2 track  2 shower\n",
    "    kNumu2trMsh=enum.auto()          # Numu CC -  2 track 3+ shower\n",
    "    kNumuMtr0sh=enum.auto()          # Numu CC - 3+ track no showe\n",
    "    kNumuMtr1sh=enum.auto()          # Numu CC - 3+ track  1 shower\n",
    "    kNumuMtr2sh=enum.auto()          # Numu CC - 3+ track  2 showe\n",
    "    kNumuMtrMsh=enum.auto()          # Numu CC - 3+ track 3+ shower\n",
    "    kNue0tr0sh=enum.auto()           # Nue CC - no track no shower\n",
    "    kNue0tr1sh=enum.auto()           # Nue CC - no track  1 shower\n",
    "    kNue0tr2sh=enum.auto()           # Nue CC - no track  2 showe\n",
    "    kNue0trMsh=enum.auto()           # Nue CC - no track 3+ shower\n",
    "    kNue1tr0sh=enum.auto()           # Nue CC -  1 track no shower\n",
    "    kNue1tr1sh=enum.auto()           # Nue CC -  1 track  1 shower\n",
    "    kNue1tr2sh=enum.auto()           # Nue CC -  1 track  2 shower\n",
    "    kNue1trMsh=enum.auto()           # Nue CC -  1 track 3+ shower\n",
    "    kNue2tr0sh=enum.auto()           # Nue CC -  2 track no shower\n",
    "    kNue2tr1sh=enum.auto()           # Nue CC -  2 track  1 shower\n",
    "    kNue2tr2sh=enum.auto()           # Nue CC -  2 track  2 shower\n",
    "    kNue2trMsh=enum.auto()           # Nue CC -  2 track 3+ shower\n",
    "    kNueMtr0sh=enum.auto()           # Nue CC - 3+ track no shower\n",
    "    kNueMtr1sh=enum.auto()           # Nue CC - 3+ track  1 shower\n",
    "    kNueMtr2sh=enum.auto()           # Nue CC - 3+ track  2 shower\n",
    "    kNueMtrMsh=enum.auto()           # Nue CC - 3+ track 3+ shower\n",
    "    kNC0tr0sh=enum.auto()           # NC CC - no track no shower\n",
    "    kNC0tr1sh=enum.auto()           # NC CC - no track  1 shower\n",
    "    kNC0tr2sh=enum.auto()           # NC CC - no track  2 shower\n",
    "    kNC0trMsh=enum.auto()           # NC CC - no track 3+ shower\n",
    "    kNC1tr0sh=enum.auto()           # NC CC -  1 track no shower\n",
    "    kNC1tr1sh=enum.auto()           # NC CC -  1 track  1 shower\n",
    "    kNC1tr2sh=enum.auto()           # NC CC -  1 track  2 shower\n",
    "    kNC1trMsh=enum.auto()           # NC CC -  1 track 3+ shower\n",
    "    kNC2tr0sh=enum.auto()           # NC CC -  2 track no shower\n",
    "    kNC2tr1sh=enum.auto()           # NC CC -  2 track  1 shower\n",
    "    kNC2tr2sh=enum.auto()           # NC CC -  2 track  2 shower\n",
    "    kNC2trMsh=enum.auto()           # NC CC -  2 track 3+ shower\n",
    "    kNCMtr0sh=enum.auto()           # NC CC - 3+ track no shower\n",
    "    kNCMtr1sh=enum.auto()           # NC CC - 3+ track  1 shower\n",
    "    kNCMtr2sh=enum.auto()           # NC CC - 3+ track  2 shower\n",
    "    kNCMtrMsh=enum.auto()           # NC CC - 3+ track 3+ shower\n",
    "    kCosmicFS=enum.auto()           # Cosmic ray background\n",
    "    kOtherFS=enum.auto()            # Something else.  Tau?  Hopefully we don't use this\n",
    "    kNFStType=enum.auto()            # Number of interaction types, used like a vector size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the urllib library\n",
    "import urllib.request\n",
    "\n",
    "# Create empty dictionary to hold data files\n",
    "data = {}\n",
    "\n",
    "# Loops to store data in dictionaries\n",
    "for i in range(1,30):\n",
    "    \n",
    "    # Copy a network object to a local file\n",
    "    urllib.request.urlretrieve('http://www.hep.ucl.ac.uk/undergrad/0056/other/projects/nova/neutrino'+str(i)+'.h5', \n",
    "                               'neutrino'+str(i)+'.h5')\n",
    "    \n",
    "    # Open the local h5 file with h5py, stores in dictionary\n",
    "    data[i] = h5py.File('neutrino'+str(i)+'.h5','r')\n",
    "    \n",
    "    # Can use exec to store as variables \n",
    "    #exec(f\"df{i} = h5py.File('neutrino{i}.h5','r')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[1]\n",
    "i  = 10\n",
    "\n",
    "# Print neutrino data meta data values\n",
    "print(Interaction(df['neutrino']['interaction'][i]))\n",
    "print(FinalState(df['neutrino']['finalstate'][i]))\n",
    "print(df['neutrino']['evt'][i])\n",
    "\n",
    "# Plot meta data as a table\n",
    "pd.DataFrame(df['neutrino'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the keys in the neutrino meta data\n",
    "print(df.keys())\n",
    "print(df['neutrino'].keys())\n",
    "\n",
    "#Get an numpy array containing the event image, and reshape it from flat to 2x100x80\n",
    "print(np.shape(df['cvnmap']))\n",
    "print(df['neutrino']['evt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract event images to plot\n",
    "event0 = np.array(df['cvnmap'][0]).reshape((2,100,80))\n",
    "\n",
    "# Plot the first event, look it is a nice long muon track\n",
    "fig, ax = plt.subplots(1,2, figsize = (15,5))\n",
    "\n",
    "im1 = ax[0].imshow(event0[0].T, cmap = 'Reds')\n",
    "ax[0].set_title('X-Z track image')\n",
    "ax[0].set_xlabel('x, arb')\n",
    "ax[0].set_ylabel('z, arb')\n",
    "\n",
    "im2 = ax[1].imshow(event0[1].T, cmap = 'Reds')\n",
    "ax[1].set_title('Y-Z track image')\n",
    "ax[1].set_xlabel('y, arb')\n",
    "ax[1].set_ylabel('z, arb')\n",
    "\n",
    "# fig.colorbar(im1, ax= ax[0])\n",
    "# fig.colorbar(im2, ax= ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Neutrino Final State code\",df['neutrino']['finalstate'][3])\n",
    "print(\"Interaction was \",Interaction(df['neutrino']['cycle'][7]))\n",
    "print(\"Neutrino energy\",df['neutrino']['finalstateprong'][5][0],\"GeV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions:\n",
    "- Creating functions to help process and hold data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classes(data, labels):\n",
    "    ''' Divides data into classes in a dictionary.\n",
    "    \n",
    "    Inputs:\n",
    "        data   - Data array containg images \n",
    "        labels - Corresponding class labels for images\n",
    "\n",
    "    Return:\n",
    "        classes - Dictionary of class stratafied data, in which keys are the classes \n",
    "        and values are corresponding images\n",
    "    '''\n",
    "    \n",
    "    # Initialise dictionary\n",
    "    classes = {}\n",
    "    \n",
    "    # for the index and values in the array\n",
    "    for i, v in enumerate(labels):\n",
    "        \n",
    "        # Ensures values are integers not arrays\n",
    "        if type(v) == np.ndarray:\n",
    "            value = v[0]\n",
    "        else:\n",
    "            value = v\n",
    "            \n",
    "        # Adds value as key of dictionary and attach corresponding data\n",
    "        if value in list(classes.keys()):\n",
    "            classes[value].append(data[i])\n",
    "\n",
    "        else:\n",
    "            classes[value] = [data[i]] \n",
    "   \n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassEqualised(data, labels):\n",
    "    ''' Ensures the arrays have equal class distributions.\n",
    "    \n",
    "    Input:\n",
    "        data   - Data array containg images \n",
    "        labels - Corresponding class labels for images\n",
    "\n",
    "    Return:\n",
    "        CE_data   - Data array containg images of equal class distribution  \n",
    "        CE_labels - Corresponding class labels for images of equal class distribution\n",
    "    \n",
    "    '''\n",
    "    # Extract Classes from data\n",
    "    classes = Classes(data, labels)\n",
    "    \n",
    "    # Random shuffle of data\n",
    "    for keys, values in list(classes.items()):\n",
    "        p = np.random.permutation(len(values))\n",
    "        classes[keys] = np.array(values)[p] \n",
    "    \n",
    "    # Find size of smallest class\n",
    "    N = np.min([len(classes[i]) for i in list(classes.keys())])\n",
    "    \n",
    "    # List comprehension to equally join each classes back to one array\n",
    "    CE_data   = np.array([ i for group in list(classes.values()) for i in group[:N] ])\n",
    "    CE_labels = np.array([ [k] for i in list(classes.keys()) for k in [i]*N ])\n",
    "    \n",
    "    # Randomly shuffle arrays, preserving labeling\n",
    "    p = np.random.permutation(len(CE_labels))\n",
    "    CE_data   = CE_data[p]\n",
    "    CE_labels = CE_labels[p]\n",
    "    \n",
    "    return CE_data, CE_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dictionary(Input_dictionary, Input_array, data_no, **bin_kwargs):\n",
    "    ''' Function to sort data in array into dictionary holding according images for each class \n",
    "    and subclass of meta data.\n",
    "    \n",
    "    Inputs:\n",
    "    Input_dictionary - Dictionary to hold  meta data strata\n",
    "    Input_array      - Array of data (images, labels, or predictions), ordered in corresponding order of data[data_no]\n",
    "    data_no          - Number of the data file corresponding to input array order \n",
    "    **bin_kwargs     - Keyword arguements used sort energy in binned strata\n",
    "    \n",
    "    Return:\n",
    "    Input_dictionary - Updated dictionary holding sorted data\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # For each meta data key in file\n",
    "    for i in data[data_no]['neutrino'].keys():\n",
    "        \n",
    "        # Ensures meta data key is in dictionary\n",
    "        if i in Input_dictionary:\n",
    "            None\n",
    "        else:\n",
    "            Input_dictionary[i] = {}\n",
    "        \n",
    "        # For the data in the array adds to correct 'sub-class' in meta data\n",
    "        for j in range(len(Input_array)):\n",
    "            \n",
    "            # Extract data 'sub-class' \n",
    "            k = data[data_no]['neutrino'][i][j][0]\n",
    "            \n",
    "            # Adds data to corresponding dictionary location\n",
    "            if k in Input_dictionary[i]:\n",
    "                Input_dictionary[i][k].append(Input_array[j]) \n",
    "            else: \n",
    "                Input_dictionary[i][k] = [Input_array[j]]\n",
    "           \n",
    "                \n",
    "    # Adds key for energy into dictionary\n",
    "    if 'energy' in Input_dictionary:\n",
    "        None\n",
    "    else:\n",
    "        Input_dictionary['energy'] = {} \n",
    "\n",
    "    # For the data in the array adds to corresponding energy key\n",
    "    for i in range(len(Input_array)):\n",
    "        \n",
    "        # Calculates corresponding image energy\n",
    "        k = data[data_no]['neutrino']['nuenergy'][i][0] + data[data_no]['neutrino']['lepenergy'][i][0]\n",
    "\n",
    "        # Adds data to corresponding dictionary location\n",
    "        if k in Input_dictionary['energy']:\n",
    "            Input_dictionary['energy'][k].append(Input_array[i])\n",
    "\n",
    "        else: \n",
    "            Input_dictionary['energy'][k] = [Input_array[i]]\n",
    "     \n",
    "    \n",
    "    # If we have bin_kwargs, adds to dictionary\n",
    "    for keys, values in bin_kwargs.items():\n",
    "        \n",
    "        # Extract data label\n",
    "        bin_data_label = str(keys)+str(values)\n",
    "        \n",
    "        # Adds label to dictionary\n",
    "        if bin_data_label in Input_dictionary:\n",
    "            None\n",
    "        else:\n",
    "            Input_dictionary[bin_data_label] = {} \n",
    "            \n",
    "        # For the data in the array adds to corresponding key\n",
    "        for i in range(len(Input_array)):\n",
    "             # Uses kwarg value to round data into stara bin, with condition for if it is energy\n",
    "            if keys == 'energy':\n",
    "                ke= data[data_no]['neutrino']['nuenergy'][i][0] + data[data_no]['neutrino']['lepenergy'][i][0]\n",
    "                k = values*round(ke/values)\n",
    "            else:\n",
    "                k = values*round(data[data_no]['neutrino'][keys][i][0]/values)\n",
    "            \n",
    "            # Adds data to corresponding dictionary location\n",
    "            if k in Input_dictionary[bin_data_label]:\n",
    "                Input_dictionary[bin_data_label][k].append(Input_array[i])\n",
    "\n",
    "            else: \n",
    "                Input_dictionary[bin_data_label][k] = [Input_array[i]]\n",
    "\n",
    "    return Input_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation Tests: \n",
    "(Note: Kernal runs out of memory later if you run this section, you have to restart kernal and run code without this section to run testing later)\n",
    "- Exploring that image values vs corresponding energies and effect of normalsation; intend to check if there is underlying correlation in pixel values and if any information is lost from normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract event images\n",
    "event  = np.array(data[1]['cvnmap'], dtype = 'float')\n",
    "event_inputs = event.reshape((event.shape[0],2,100,80)).transpose(0,2,3,1)\n",
    "\n",
    "# Extract event images and normalise\n",
    "event_norm  = np.array(data[1]['cvnmap'], dtype = 'float')\n",
    "event_norm /= event_norm.max()\n",
    "event_norm /= event_norm.max()\n",
    "event_inputs_norm = event_norm.reshape((event_norm.shape[0],2,100,80)).transpose(0,2,3,1)\n",
    "\n",
    "# Extract event images and alternativenormalise\n",
    "event_norm2  = np.array(data[1]['cvnmap'], dtype = 'float')\n",
    "event_norm2 /= (event_norm2.max(axis=1)[:,None])\n",
    "event_norm2 /= (event_norm2.max(axis=1)[:,None])\n",
    "event_inputs_norm2 = event_norm2.reshape((event_norm2.shape[0],2,100,80)).transpose(0,2,3,1)\n",
    "\n",
    "# Create meta data dictionary\n",
    "Inputs_dic       =  Dictionary({}, event_inputs,  1)\n",
    "Inputs_dic_norm  =  Dictionary({}, event_inputs_norm, 1)\n",
    "Inputs_dic_norm2 =  Dictionary({}, event_inputs_norm2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Energy data keys\n",
    "key, key_norm, key_norm2, = (np.array(list(Inputs_dic['nuenergy'].keys())), \n",
    "                             np.array(list(Inputs_dic_norm['nuenergy'].keys())),\n",
    "                             np.array(list(Inputs_dic_norm2['nuenergy'].keys()))  )\n",
    "\n",
    "# Initialise arrays to hold pixel values\n",
    "ave, ave_norm, ave_norm2 = np.zeros([len(key),2]), np.zeros([len(key),2]), np.zeros([len(key),2])\n",
    "mxm, mxm_norm, mxm_norm2 = np.zeros([len(key),2]), np.zeros([len(key),2]), np.zeros([len(key),2])\n",
    "\n",
    "# Take max and mean value for each corresponding energy\n",
    "for i in range(len(Inputs_dic['nuenergy'])):   \n",
    "    ave[i][1], ave[i][0] = np.mean(Inputs_dic['nuenergy'][key[i]]), key[i]\n",
    "    mxm[i][1], mxm[i][0] = np.max(Inputs_dic['nuenergy'][key[i]]),  key[i]\n",
    "      \n",
    "# Take max and mean value for each corresponding energy, for normalised data\n",
    "for i in range(len(Inputs_dic_norm['nuenergy'])): \n",
    "    ave_norm[i][1], ave_norm[i][0] = np.mean(Inputs_dic_norm['nuenergy'][key_norm[i]]), key_norm[i]    \n",
    "    mxm_norm[i][1], mxm_norm[i][0] = np.max(Inputs_dic_norm['nuenergy'][key_norm[i]]),  key_norm[i]\n",
    "\n",
    "# Take max and mean value for each corresponding energy, for normalised 2 data\n",
    "for i in range(len(Inputs_dic_norm2['nuenergy'])): \n",
    "    ave_norm2[i][1], ave_norm2[i][0] = np.mean(Inputs_dic_norm2['nuenergy'][key_norm2[i]]), key_norm2[i]    \n",
    "    mxm_norm2[i][1], mxm_norm2[i][0] = np.max(Inputs_dic_norm2['nuenergy'][key_norm2[i]]),  key_norm2[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot corresponding plots of max/mean pixel value against energy\n",
    "fig, ax = plt.subplots(3,2, figsize=(20,10))\n",
    "\n",
    "ax[0,0].scatter(ave[:,0],ave[:,1], color= 'k')\n",
    "ax[0,0].set_title('Average pixel value against energy for un-normalised data')\n",
    "ax[0,0].set_xlabel('Energy, eV')\n",
    "ax[0,0].set_ylabel('Average pixel value, arb')\n",
    "\n",
    "ax[0,1].scatter(mxm[:,0],mxm[:,1], color= 'k')\n",
    "ax[0,1].set_title('Maximum pixel value against energy for un-normalised data')\n",
    "ax[0,1].set_xlabel('Energy, eV')\n",
    "ax[0,1].set_ylabel('Maximum pixel value, arb')\n",
    "\n",
    "ax[1,0].scatter(ave_norm[:,0],ave_norm[:,1], color= 'k')\n",
    "ax[1,0].set_ylim(-0.001, 0.008)\n",
    "ax[1,0].set_title('Average pixel value against energy for batch normalised data')\n",
    "ax[1,0].set_xlabel('Energy, eV')\n",
    "ax[1,0].set_ylabel('Average pixel value, arb')\n",
    "\n",
    "ax[1,1].scatter(mxm_norm[:,0],mxm_norm[:,1], color= 'k')\n",
    "ax[1,1].set_title('Maximum pixel value against energy for batch normalised data')\n",
    "ax[1,1].set_xlabel('Energy, eV')\n",
    "ax[1,1].set_ylabel('Maximum pixel value, arb')\n",
    "\n",
    "ax[2,0].scatter(ave_norm2[:,0],ave_norm2[:,1], color= 'k')\n",
    "ax[2,0].set_ylim(-0.001, 0.015)\n",
    "ax[2,0].set_title('Average pixel value against energy for individually normalised data')\n",
    "ax[2,0].set_xlabel('Energy, eV')\n",
    "ax[2,0].set_ylabel('Average pixel value, arb')\n",
    "\n",
    "ax[2,1].scatter(mxm_norm2[:,0],mxm_norm2[:,1], color= 'k')\n",
    "ax[2,1].set_title('Maximum pixel value against energy for individually normalised data')\n",
    "ax[2,1].set_xlabel('Energy, eV')\n",
    "ax[2,1].set_ylabel('Maximum pixel value, arb')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising arrays\n",
    "maxpix = np.zeros(len(event))\n",
    "avepix = np.zeros(len(event))\n",
    "\n",
    "maxpix_norm = np.zeros(len(event_norm))\n",
    "avepix_norm = np.zeros(len(event_norm))\n",
    "\n",
    "# Storing average and maximum over\n",
    "for i in range(len(event_norm)):\n",
    "    maxpix[i]= np.max(event_inputs[i])\n",
    "    avepix[i]= np.mean(event_inputs[i])\n",
    "    \n",
    "    maxpix_norm[i]= np.max( event_inputs_norm[i])\n",
    "    avepix_norm[i]= np.mean(event_inputs_norm[i])\n",
    "\n",
    "# Extract energy arrays from data\n",
    "energy   = np.array(data[1]['neutrino']['nuenergy']) + np.array(data[1]['neutrino']['lepenergy'])\n",
    "nuenergy = np.array(data[1]['neutrino']['nuenergy'])\n",
    "lepenergy= np.array(data[1]['neutrino']['lepenergy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting Histograms of data\n",
    "fig, ax = plt.subplots(3,2, figsize=(15,15))\n",
    "\n",
    "ax[0,0].hist(nuenergy, bins=30, color= 'k')\n",
    "ax[0,0].set_title('Histogram of image Nu-Energies')\n",
    "ax[0,0].set_xlabel('Energy, eV')\n",
    "ax[0,0].set_ylabel('No. of Images')\n",
    "\n",
    "ax[0,1].hist(lepenergy, bins=30, color= 'k')\n",
    "ax[0,1].set_title('Histogram of image Lep-Energies')\n",
    "ax[0,1].set_xlabel('Energy, eV')\n",
    "ax[0,1].set_ylabel('No. of Images')\n",
    "\n",
    "ax[1,0].hist(maxpix, bins=30, color= 'k')\n",
    "ax[1,0].set_title('Histogram of Maximum Pixel Value')\n",
    "ax[1,0].set_xlabel('Pixel value, arb')\n",
    "ax[1,0].set_ylabel('No. of Images')\n",
    "\n",
    "ax[1,1].hist(avepix, bins=30, color= 'k')\n",
    "ax[1,1].set_title('Histogram of Average Pixel Value')\n",
    "ax[1,1].set_xlabel('Pixel value, arb')\n",
    "ax[1,1].set_ylabel('No. of Images')\n",
    "\n",
    "ax[2,0].hist(maxpix_norm, bins=30, color= 'k')\n",
    "ax[2,0].set_title('Histogram of Maximum Normalised Pixel Value')\n",
    "ax[2,0].set_xlabel('Pixel value, arb')\n",
    "ax[2,0].set_ylabel('No. of Images')\n",
    "\n",
    "ax[2,1].hist(avepix_norm, bins=30, color= 'k')\n",
    "ax[2,1].set_title('Histogram of Average Normalised Pixel Value')\n",
    "ax[2,1].set_xlabel('Pixel value, arb')\n",
    "ax[2,1].set_ylabel('No. of Images')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- We can see that normalising images removes shape to the the distibutions with respect to pixel value, so underlying information over the group of images as a whole could be lost; any gains in computational speed would be negated, as the relative pixel brightness/ variation may hold information on energy of interaction; to ensure underlying correlation in results is preserved we will normalise by maximum value over whole batch, not individual images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Images of Interaction types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 34\n",
    "\n",
    "#Plot the event from training data\n",
    "fig, ax = plt.subplots(1,3, figsize=(20,5))\n",
    "im1 = ax[0].imshow(Inputs_dic['interaction'][0][index][:,:,0].T, cmap = 'Reds')\n",
    "ax[0].set_title('QE interaction track image')\n",
    "ax[0].set_xlabel('z, arb')\n",
    "ax[0].set_ylabel('x, arb')\n",
    "\n",
    "im2 = ax[1].imshow(Inputs_dic['interaction'][1][index][:,:,0].T, cmap = 'Reds')\n",
    "ax[1].set_title('Res interaction track image')\n",
    "ax[1].set_xlabel('z, arb')\n",
    "ax[1].set_ylabel('x, arb')\n",
    "\n",
    "im3 = ax[2].imshow(Inputs_dic['interaction'][2][index][:,:,0].T, cmap = 'Reds')\n",
    "ax[2].set_title('DIS interaction track image')\n",
    "ax[2].set_xlabel('z, arb')\n",
    "ax[2].set_ylabel('x, arb')\n",
    "\n",
    "# fig.colorbar(im1, ax= ax[0])\n",
    "# fig.colorbar(im2, ax= ax[1])\n",
    "# fig.colorbar(im3, ax= ax[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 3\n",
    "print(Inputs_dic['interaction'].keys())\n",
    "#Plot the event from training data\n",
    "fig, ax = plt.subplots(1,3, figsize=(20,5))\n",
    "im1 = ax[0].imshow(Inputs_dic['interaction'][0][index][:,:,0].T, cmap = 'Reds')\n",
    "ax[0].set_title('CC NuMu interaction track image')\n",
    "ax[0].set_xlabel('z, arb')\n",
    "ax[0].set_ylabel('x, arb')\n",
    "\n",
    "im2 = ax[1].imshow(Inputs_dic['interaction'][4][index][:,:,0].T, cmap = 'Reds')\n",
    "ax[1].set_title('CC NuE interaction track image')\n",
    "ax[1].set_xlabel('z, arb')\n",
    "ax[1].set_ylabel('x, arb')\n",
    "\n",
    "im3 = ax[2].imshow(Inputs_dic['interaction'][13][index][:,:,0].T, cmap = 'Reds')\n",
    "ax[2].set_title('CC NuTau interaction track image')\n",
    "ax[2].set_xlabel('z, arb')\n",
    "ax[2].set_ylabel('x, arb')\n",
    "\n",
    "# fig.colorbar(im1, ax= ax[0])\n",
    "# fig.colorbar(im2, ax= ax[1])\n",
    "# fig.colorbar(im3, ax= ax[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture Exploration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Training Data:\n",
    "Concatenate data to create larger batch. Here we have to use Class Equalisation function as imported data containes unbalanced classes, as shown in cell before balancing there are 88% ones and only 12% zeros; this causes miss fitting and stagnation of models as the network can reduce lost by predicting all ones and fall into false minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract initial data and label information (use uint8 to useless memory)\n",
    "event  = np.array(data[1]['cvnmap'], dtype='uint8')\n",
    "event_l= np.array(data[1]['neutrino']['interaction'],dtype='uint8')\n",
    "\n",
    "# Loop to join data into arrays to get larger batch\n",
    "for i in range(2,10):\n",
    "    event  = np.concatenate(( event   , np.array(data[i]['cvnmap'], dtype = 'uint8') ))  \n",
    "    event_l= np.concatenate(( event_l , np.array(data[i]['neutrino']['interaction'], dtype='uint8') )) \n",
    "\n",
    "# Normalise images\n",
    "# event /= event.max()\n",
    "\n",
    "# Reshape inputs into correct shape for networks\n",
    "event_inputs = event.reshape((event.shape[0],2,100,80)).transpose(0,2,3,1)\n",
    "\n",
    "# Format labels such that Numu are 1, and rest are 0\n",
    "event_labels= np.where( event_l > 3, 0, 1).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class dictionary\n",
    "classes = Classes(event_inputs, event_labels)\n",
    "\n",
    "# Extract number of each class\n",
    "a = len(list(classes.values())[0])\n",
    "b = len(list(classes.values())[1])\n",
    "\n",
    "# Print percentage of each class\n",
    "print('Shape of data', event_inputs.shape)\n",
    "print('Percentage of', list(classes.keys())[0], 'is', a/(a+b))\n",
    "print('Percentage of', list(classes.keys())[1], 'is', b/(a+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equalise class data\n",
    "train_event_inputs, train_event_labels = ClassEqualised(event_inputs, event_labels)\n",
    "print('Shape of training data', train_event_inputs.shape)\n",
    "\n",
    "# Create class dictionary\n",
    "classes = Classes(train_event_inputs, train_event_labels)\n",
    "\n",
    "# Extract number of each class\n",
    "a = len(list(classes.values())[0])\n",
    "b = len(list(classes.values())[1])\n",
    "\n",
    "# Print percentage of each class\n",
    "print('Percentage of after class equalising', list(classes.keys())[0], 'is', a/(a+b))\n",
    "print('Percentage of after class equalising', list(classes.keys())[1], 'is', b/(a+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking images are preserved\n",
    "index = 14\n",
    "\n",
    "#Plot the event from training data\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
    "im1 = ax[0].imshow(train_event_inputs[index][:,:,0].T, cmap = 'Reds')\n",
    "ax[0].set_title('X-Z track image')\n",
    "ax[0].set_xlabel('z, arb')\n",
    "ax[0].set_ylabel('x, arb')\n",
    "\n",
    "im2 = ax[1].imshow(train_event_inputs[index][:,:,1].T, cmap = 'Reds')\n",
    "ax[1].set_title('Y-Z track image')\n",
    "ax[1].set_xlabel('z, arb')\n",
    "ax[1].set_ylabel('y, arb')\n",
    "\n",
    "# fig.colorbar(im1, ax= ax[0])\n",
    "# fig.colorbar(im2, ax= ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1:\n",
    "Seperable conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Convolutional neural network\n",
    "test_model1 = keras.models.Sequential()\n",
    "\n",
    "# First, Seperable convolutional layer of 32, 7x7 kernals; Pooling 2x2 and Spatial dropout for overfitting\n",
    "test_model1.add(keras.layers.SeparableConv2D(32, 7, activation='relu', input_shape=(100,80,2)))\n",
    "test_model1.add(keras.layers.MaxPooling2D((2, 2), strides = 2))\n",
    "test_model1.add(keras.layers.SpatialDropout2D(0.3))\n",
    "\n",
    "# Second convolutional layer of 32, 3x3 kernals; Pooling 2x2  and dropout for overfitting\n",
    "test_model1.add(keras.layers.Conv2D(64, 3, activation='relu', input_shape=(100,80,2)))\n",
    "test_model1.add(keras.layers.MaxPooling2D((2, 2), strides = 2))\n",
    "test_model1.add(keras.layers.Dropout(0.3))\n",
    "\n",
    "# model.add(keras.layers.SeparableConv2D(128, 3, activation='relu', input_shape=(100,80,2)))\n",
    "# model.add(keras.layers.MaxPooling2D((2, 2), strides = 1))\n",
    "\n",
    "# Third convolutional layer of 64, 3x3 kernals \n",
    "test_model1.add(keras.layers.Conv2D(64, 3, activation='relu'))\n",
    "test_model1.add(keras.layers.Dropout(0.7))\n",
    "\n",
    "# Fully connected Neural Network\n",
    "test_model1.add(keras.layers.Flatten())\n",
    "test_model1.add(keras.layers.Dense(128, activation='relu'))\n",
    "test_model1.add(keras.layers.Dropout(0.7))\n",
    "test_model1.add(keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "# We have a dropout layer to prevent overfitting\n",
    "test_model1.add(keras.layers.Dropout(0.7))\n",
    "test_model1.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# Compile the network with binary crossentropy loss and adam optimiser with learning rate of 1.0\n",
    "test_model1.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "              loss      = tf.keras.losses.BinaryCrossentropy(from_logits=False), \n",
    "              metrics   = [keras.metrics.BinaryAccuracy()] )\n",
    "\n",
    "test_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "test_history1 = test_model1.fit(train_event_inputs, train_event_labels, validation_split=0.33,\n",
    "                                batch_size =100, epochs=25, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting accuracy over epochs\n",
    "fig,ax=plt.subplots(1,2, figsize=(15,5))\n",
    "ax[0].plot(test_history1.history['binary_accuracy'], linewidth=3)\n",
    "ax[0].plot(test_history1.history['val_binary_accuracy'], linewidth=3)\n",
    "\n",
    "# Setting axis and labels\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_title('Plot of Accuray over training Epoch')\n",
    "\n",
    "ax[1].plot(test_history1.history['loss'], linewidth=3)\n",
    "ax[1].plot(test_history1.history['val_loss'], linewidth=3)\n",
    "\n",
    "# Setting axis and labels\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_title('Plot of Accuray over training Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- We get good training curves and good validation accuracy, showing no overfitting, however,, the validation values are eratic and drop below training values, will combine with further layers to seperate curves more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2:\n",
    "The track images show movement through space, each verticle line of pixels show trackes subsequent movement. We have implimented a LSTM layer to see if they can be some correlation found in the psuedo-causality described, we then use a convolutional layers for the featue learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First input branch\n",
    "x_input = Input(shape=(100, 80))\n",
    "\n",
    "# LSTM layer, increased dimensions to then convolute\n",
    "x_lstm   = keras.layers.LSTM(40, return_sequences=True)(x_input)\n",
    "x_lstm   = tf.expand_dims(x_lstm, axis=-1)\n",
    "\n",
    "# Convolutional and Pooling Layer for feature learning\n",
    "x_conv2d = keras.layers.Conv2D(32, 3, padding='same')(x_lstm)\n",
    "x_pool2d = keras.layers.MaxPool2D((2,2), strides = 2, padding='same')(x_conv2d)\n",
    "\n",
    "\n",
    "# Second input branch\n",
    "y_input = Input(shape=(100, 80))\n",
    "\n",
    "# LSTM layer, increased dimensions to then convolute\n",
    "y_lstm   = keras.layers.LSTM(40, return_sequences=True)(y_input)\n",
    "y_lstm   = tf.expand_dims(y_lstm, axis=-1)\n",
    "\n",
    "# Convolutional and Pooling Layer for feature learning\n",
    "y_conv2d = keras.layers.Conv2D(32, 3, padding='same')(y_lstm)\n",
    "y_pool2d = keras.layers.MaxPool2D((2,2), padding='same')(y_conv2d)\n",
    "\n",
    "# Joining branches \n",
    "xy_l1 = concatenate([x_pool2d, y_pool2d], axis=-1)\n",
    "xy_l2 = keras.layers.Conv2D(64, 2, padding='same')(xy_l1)\n",
    "\n",
    "# Fully connected Neural Network\n",
    "xy_l3 = keras.layers.Flatten()(xy_l2)\n",
    "xy_l4 = keras.layers.Dense(128, activation = 'relu')(xy_l3)\n",
    "xy_l5 = keras.layers.Dropout(0.5)(xy_l4)\n",
    "\n",
    "# Output layer\n",
    "output_layer = keras.layers.Dense(1)(xy_l5)\n",
    "\n",
    "\n",
    "test_model2 = Model(inputs= (x_input,y_input), outputs=output_layer)\n",
    "\n",
    "# Compile the network with binary crossentropy loss and adam optimiser with learning rate of 1.0\n",
    "test_model2.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                    loss      = tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "                    metrics   = [keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "test_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "test_history2 = test_model2.fit((train_event_inputs[:,:,:,0], train_event_inputs[:,:,:,1]), train_event_labels, \n",
    "                          validation_split=0.33,batch_size =100, epochs=10, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting accuracy over epochs\n",
    "fig,ax=plt.subplots(1,2, figsize=(15,5))\n",
    "ax[0].plot(test_history2.history['binary_accuracy'], linewidth=3)\n",
    "ax[0].plot(test_history2.history['val_binary_accuracy'], linewidth=3)\n",
    "\n",
    "# Setting axis and labels\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_title('Plot of Accuray over training Epoch')\n",
    "\n",
    "ax[1].plot(test_history2.history['loss'], linewidth=3)\n",
    "ax[1].plot(test_history2.history['val_loss'], linewidth=3)\n",
    "\n",
    "# Setting axis and labels\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_title('Plot of Accuray over training Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "- We see the training curves show good exponential trends, however, the validation data shows plateau and validation loss diverges, this shows the model is overfitting and not applicable to this data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3:\n",
    "Following model similar to that seen on paper 'fermilab-pub-16-082-nd.pdf' and using an inception model originally introduced in Going Deeper With Convolutions\". We recreate similar to see its computation power and if there is any architecture we can adopt. An inception layer is a combination of convolutional layers (namely, 1x1 Convolutional layer, 3x3 Convolutional layer, 5x5 Convolutional layer) with their output filter banks concatenated into a single output vector forming the input of the next stage. Here we impliment a inception module with dimensionality reduction by including additional 1x1 convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Setting up Inception architecture\n",
    "def inception_module(layer_in, k1, k3, k5, kp):\n",
    "    # 1x1 conv\n",
    "    conv1 = Conv2D(k1, (1,1), padding='same', activation='relu')(layer_in)\n",
    "    \n",
    "    # 3x3 conv\n",
    "    conv3 = Conv2D(k3, (1,1), padding='same', activation='relu')(layer_in)\n",
    "    conv3 = Conv2D(k3, (3,3), padding='same', activation='relu')(conv3)\n",
    "    \n",
    "    # 5x5 conv\n",
    "    conv5 = Conv2D(k5, (1,1), padding='same', activation='relu')(layer_in)\n",
    "    conv5 = Conv2D(k5, (5,5), padding='same', activation='relu')(conv5)\n",
    "    \n",
    "    # Pooling\n",
    "    convp = MaxPooling2D((2,2), strides = 1, padding='same')(layer_in)\n",
    "    convp = Conv2D(kp, (1,1), padding='same', activation='relu')(convp)\n",
    " \n",
    "    # concatenate filters, assumes filters/channels last\n",
    "    layer_out = concatenate([conv1, conv3, conv5, convp], axis=-1)\n",
    "\n",
    "    return layer_out\n",
    "\n",
    "####################################################################\n",
    "\n",
    "# First input branch\n",
    "x_input = Input(shape=(100, 80, 1))\n",
    "\n",
    "# First convolutional layer of 32, 7x7 kernals; Pooling 3x3 with padding and Batch Normalisation\n",
    "x_l1 = Conv2D(32, (7,7),strides= 2, padding='same', activation='relu')(x_input)\n",
    "x_l2 = MaxPooling2D((3, 3),strides= 2, padding='same')(x_l1)\n",
    "x_l3 = keras.layers.BatchNormalization()(x_l2)\n",
    "\n",
    "# Second convolutional layer of 32, 1x1 kernals\n",
    "x_l4 = Conv2D(32, (1,1), strides= 2, padding='same', activation='relu')(x_l3)\n",
    "\n",
    "# Third convolutional layer of 64, 3x3 kernals\n",
    "x_l5 = Conv2D(64, (3,3),strides= 2, padding='same', activation='relu')(x_l4)\n",
    "x_l6 = keras.layers.BatchNormalization()(x_l5)\n",
    "x_l7 = MaxPooling2D((3, 3),strides= 2, padding='same')(x_l6)\n",
    "\n",
    "# Inception modules and pooling, kernals followed from paper above\n",
    "x_l8 = inception_module(x_l7,64, 96, 16,32)\n",
    "x_l9 = inception_module(x_l8, 128, 128, 32, 64)\n",
    "x_l10 = MaxPooling2D((3, 3),strides= 2, padding='same')(x_l9)\n",
    "x_l11 = inception_module(x_l10, 128, 128, 32, 64)\n",
    "\n",
    "####################################################################\n",
    "\n",
    "# Second input branch\n",
    "y_input = Input(shape=(100, 80, 1))\n",
    "\n",
    "# First convolutional layer of 32, 7x7 kernals; Pooling 3x3 with padding and Batch Normalisation\n",
    "y_l1 = Conv2D(32, (7,7),strides= 2, padding='same', activation='relu')(y_input)\n",
    "y_l2 = MaxPooling2D((3, 3),strides= 2, padding='same')(y_l1)\n",
    "y_l3 = keras.layers.BatchNormalization()(y_l2)\n",
    "\n",
    "# Second convolutional layer of 32, 1x1 kernals\n",
    "y_l4 = Conv2D(32, (1,1), strides= 2, padding='same', activation='relu')(y_l3)\n",
    "\n",
    "# Third convolutional layer of 64, 3x3 kernals\n",
    "y_l5 = Conv2D(64, (3,3),strides= 2, padding='same', activation='relu')(y_l4)\n",
    "y_l6 = keras.layers.BatchNormalization()(y_l5)\n",
    "y_l7 = MaxPooling2D((3, 3),strides= 2, padding='same')(y_l6)\n",
    "\n",
    "# Inception modules and pooling, kernals followed from paper above\n",
    "y_l8 = inception_module(y_l7, 64, 96, 16, 32)\n",
    "y_l9 = inception_module(y_l8, 128, 128, 32, 64)\n",
    "y_l10 = MaxPooling2D((3, 3),strides= 2, padding='same')(y_l9)\n",
    "y_l11 = inception_module(y_l10, 128, 128, 32, 64)\n",
    "\n",
    "####################################################################\n",
    "\n",
    "# Joining branches of network together\n",
    "xy_l1 = concatenate([x_l11, y_l11], axis=-1)\n",
    "xy_l2 = inception_module(xy_l1, 192, 96, 16, 64)\n",
    "xy_l3 =  keras.layers.AveragePooling2D((6,5),padding='same')(xy_l2)\n",
    "\n",
    "# Fully connected neural network\n",
    "xy_l4 = keras.layers.Flatten()(xy_l3)\n",
    "xy_l5 = keras.layers.Dense(64, activation='relu')(xy_l4)\n",
    "xy_l6 = keras.layers.Dropout(0.3)(xy_l5)\n",
    "\n",
    "# Output layer\n",
    "output_layer = keras.layers.Dense(1)(xy_l6)\n",
    "\n",
    "# Create model\n",
    "test_model3 = Model(inputs= (x_input, y_input), outputs=output_layer)\n",
    "\n",
    "# Compile the network with binary crossentropy loss and adam optimiser with learning rate of 1.0\n",
    "test_model3.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                    loss      = tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "                    metrics   = [keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "test_model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "test_history3 = test_model3.fit((train_event_inputs[:,:,:,0], train_event_inputs[:,:,:,1]), train_event_labels, \n",
    "                          validation_split=0.33,batch_size=100, epochs=15, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting accuracy over epochs\n",
    "fig,ax=plt.subplots(1,2, figsize=(15,5))\n",
    "ax[0].plot(test_history3.history['binary_accuracy'], linewidth=3)\n",
    "ax[0].plot(test_history3.history['val_binary_accuracy'], linewidth=3)\n",
    "\n",
    "# Setting axis and labels\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_title('Plot of Accuray over training Epoch')\n",
    "\n",
    "ax[1].plot(test_history3.history['loss'], linewidth=3)\n",
    "ax[1].plot(test_history3.history['val_loss'], linewidth=3)\n",
    "\n",
    "# Setting axis and labels\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_title('Plot of Accuray over training Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- We get good validation accuracy at the begining comparitively, however, model wildly over fits and no characteristic learning curve, the inception module was taken and similar architecture was used on a simplified model, with dropout layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Binary Classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Binary Convolutional Model:\n",
    "Model decided is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Setting up Inception architecture\n",
    "def inception_module(layer_in, k1, k3, k5, kp):\n",
    "    # 1x1 conv\n",
    "    conv1 = Conv2D(k1, (1,1), padding='same', activation='relu')(layer_in)\n",
    "    \n",
    "    # 3x3 conv\n",
    "    conv3 = Conv2D(k3, (1,1), padding='same', activation='relu')(layer_in)\n",
    "    conv3 = Conv2D(k3, (3,3), padding='same', activation='relu')(conv3)\n",
    "    \n",
    "    # 5x5 conv\n",
    "    conv5 = Conv2D(k5, (1,1), padding='same', activation='relu')(layer_in)\n",
    "    conv5 = Conv2D(k5, (5,5), padding='same', activation='relu')(conv5)\n",
    "    \n",
    "    # Pooling\n",
    "    convp = MaxPooling2D((2,2), strides = 1, padding='same')(layer_in)\n",
    "    convp = Conv2D(kp, (1,1), padding='same', activation='relu')(convp)\n",
    " \n",
    "    # concatenate filters, assumes filters/channels last\n",
    "    layer_out = concatenate([conv1, conv3, conv5, convp], axis=-1)\n",
    "\n",
    "    return layer_out\n",
    "\n",
    "####################################################################\n",
    "\n",
    "# Input branch\n",
    "inputs = Input(shape=(100, 80, 2))\n",
    "\n",
    "# First, Seperable convolutional layer of 32, 7x7 kernals; Pooling 2x2 and Spatial dropout for overfitting\n",
    "conv1 = keras.layers.SeparableConv2D(32, 7, activation='relu', input_shape=(100,80,2))(inputs)\n",
    "pool1 = keras.layers.MaxPooling2D((2, 2), strides = 2)(conv1)\n",
    "drop1 = keras.layers.SpatialDropout2D(0.4)(pool1)\n",
    "\n",
    "# Second convolutional layer of 32, 3x3 kernals; Pooling 2x2  and dropout for overfitting\n",
    "conv2 = keras.layers.Conv2D(64, 3, activation='relu', input_shape=(100,80,2))(drop1)\n",
    "pool2 = keras.layers.MaxPooling2D((2, 2), strides = 2)(conv2)\n",
    "drop2 = keras.layers.Dropout(0.4)(pool2)\n",
    "\n",
    "# Inception layer\n",
    "incp1 = inception_module(drop2,64, 96, 16,32)\n",
    "\n",
    "# Third convolutional layer of 64, 3x3 kernals \n",
    "conv3 = keras.layers.Conv2D(64, 3, activation='relu')(incp1)\n",
    "drop3 = keras.layers.Dropout(0.7)(conv3)\n",
    "\n",
    "# Fully connected Neural Network\n",
    "flatn = keras.layers.Flatten()(drop3)\n",
    "dens1 = keras.layers.Dense(128, activation='relu')(flatn)\n",
    "drop4 = keras.layers.Dropout(0.8)(dens1)\n",
    "dens2 = keras.layers.Dense(64, activation='relu')(drop4)\n",
    "\n",
    "# We have a dropout layer to prevent overfitting\n",
    "drop = keras.layers.Dropout(0.8)(dens2)\n",
    "\n",
    "# Create output layer\n",
    "output_layer = keras.layers.Dense(1, activation = 'sigmoid')(drop)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs= inputs, outputs=output_layer)\n",
    "\n",
    "# Compile the network with binary crossentropy loss and adam optimiser with learning rate of 1.0\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                    loss      = tf.keras.losses.BinaryCrossentropy(from_logits=False), \n",
    "                    metrics   = [keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# history = model.fit((train_event_inputs[:,:,:,0], train_event_inputs[:,:,:,1]), train_event_labels, \n",
    "#                     validation_split=0.33, batch_size =100, epochs=15, verbose = 2)\n",
    "\n",
    "history = model.fit( train_event_inputs, train_event_labels, \n",
    "                     validation_split=0.33, batch_size =100, epochs=35, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting accuracy over epochs\n",
    "fig,ax=plt.subplots(1,2, figsize=(15,5))\n",
    "ax[0].plot(history.history['binary_accuracy'], linewidth=1, label = 'binary_accuracy')\n",
    "ax[0].plot(history.history['val_binary_accuracy'],'--', linewidth=1, label = 'val_binary_accuracy')\n",
    "\n",
    "# Setting axis and labels\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_title('Plot of Accuray over training Epoch')\n",
    "ax[0].legend()\n",
    " \n",
    "ax[1].plot(history.history['loss'], linewidth=1,label = 'loss')\n",
    "ax[1].plot(history.history['val_loss'],'--',linewidth=1,label = 'val_loss')\n",
    "\n",
    "# Setting axis and labels\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_title('Plot of Loss over training Epoch')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model Performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create testing data in same way as training data\n",
    "event  = np.array(data[10]['cvnmap'], dtype='uint8')\n",
    "event_l= np.array(data[10]['neutrino']['interaction'],dtype='uint8')\n",
    "\n",
    "for i in range(11,21):\n",
    "    event  = np.concatenate(( event   , np.array(data[i]['cvnmap'], dtype = 'uint8') ))  \n",
    "    event_l= np.concatenate(( event_l , np.array(data[i]['neutrino']['interaction'], dtype='uint8') )) \n",
    "\n",
    "# Normalisation of images\n",
    "# event /= np.max(event)\n",
    "# event /= np.max(event)\n",
    "    \n",
    "# Test data with unequal data\n",
    "test_event_inputs = event.reshape((event.shape[0],2,100,80)).transpose(0,2,3,1)\n",
    "test_event_labels = np.where( event_l > 3, 0, 1).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Equalise test data and store seperately\n",
    "test_event_inputs_eq, test_event_labels_eq = ClassEqualised(test_event_inputs, test_event_labels)\n",
    "\n",
    "# Test accuracy of trained network\n",
    "test_loss, test_acc = model.evaluate(test_event_inputs, test_event_labels, verbose=0)\n",
    "test_loss_eq, test_acc_eq = model.evaluate(test_event_inputs_eq, test_event_labels_eq, verbose=0)\n",
    "\n",
    "print('The accuracy of the network on unbalanced data is', test_acc)\n",
    "print('The accuracy of the network on balanced data is', test_acc_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- We see model losses accuracy when test on whole data set, when equalised we recover 83% accuracy, we will use `model.predict` to further explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model.predict produce array of predictions (percentage of certainties) \n",
    "y_score = model.predict(test_event_inputs, batch_size = 100)\n",
    "# True values are the labels\n",
    "y_true  = test_event_labels\n",
    "\n",
    "# Array of equalised data prediction\n",
    "y_score_eq = model.predict(test_event_inputs_eq, batch_size = 100)\n",
    "# True values are the labels\n",
    "y_true_eq  = test_event_labels_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class dictionary\n",
    "test_classes = Classes(test_event_inputs, test_event_labels)\n",
    "\n",
    "# Plotting Histogram of predictions\n",
    "fig,ax=plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "# Non-equalised Predictions\n",
    "ax[0].hist(test_event_labels.astype(int), label = 'Labels')\n",
    "ax[0].hist(y_score, label = 'Predictions')\n",
    "\n",
    "# Setting axis and labels\n",
    "ax[0].set_xlabel('Predicted Values')\n",
    "ax[0].set_ylabel('Frequencies')\n",
    "ax[0].set_title('Histogram of Predictions on Unbalanced data')\n",
    "ax[0].legend()\n",
    "\n",
    "# Equalised data prediction\n",
    "ax[1].hist(test_event_labels_eq.astype(int), label = 'Labels')\n",
    "ax[1].hist(y_score_eq, label = 'Predictions')\n",
    "\n",
    "# Setting axis and labels\n",
    "ax[1].set_xlabel('Predicted Values')\n",
    "ax[1].set_ylabel('Frequencies')\n",
    "ax[1].set_title('Histogram of Predictions on Balanced data')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- These histograms show bias in the predictions for the unbalanced data as far more predictions are given as true compared to false; this aligns with the drop in predictions as a large percentage of these ones are false positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC and Threshold Exploration:\n",
    "A receiver operating characteristic curve, ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The ROC curve summarizes the trade-off between the true positive rate and false positive rate for a predictive model using different probability thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roc curve data and false positives/ true positive rate, using sklearn function\n",
    "fpr, tpr, threshold  =  metrics.roc_curve(y_true, y_score)\n",
    "fpr_eq, tpr_eq, threshold_eq  =  metrics.roc_curve(y_true_eq, y_score_eq)\n",
    "\n",
    "# Plotting ROC and rates for unbalanced data\n",
    "fig,ax=plt.subplots(1,2, figsize=(15,5))\n",
    "ax[0].plot( fpr, tpr, label = 'ROC curve')\n",
    "ax[0].plot( threshold, fpr,'--', label = 'True False rate')\n",
    "ax[0].plot( threshold, tpr, ':', label = 'True Positive rate')\n",
    "ax[0].set_xlim(0,1)\n",
    "\n",
    "# Setting axis and labels\n",
    "ax[0].set_xlabel('False Positive Rate/ Threshold')\n",
    "ax[0].set_ylabel('True Positive Rate/ Rates')\n",
    "ax[0].set_title('Plot of ROC curve and False/True Positive rate for Unbalanced data')\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot for balanced data\n",
    "ax[1].plot( fpr_eq, tpr_eq, label = 'ROC curve')\n",
    "ax[1].plot( threshold_eq, fpr_eq,'--', label = 'True False rate')\n",
    "ax[1].plot( threshold_eq, tpr_eq, ':', label = 'True Positive rate')\n",
    "ax[1].set_xlim(0,1)\n",
    "\n",
    "# Setting axis and labels\n",
    "ax[1].set_xlabel('False Positive Rate/ Threshold')\n",
    "ax[1].set_ylabel('True Positive Rate/ Rates')\n",
    "ax[1].set_title('Plot of ROC curve and False/True Positive rate for Balanced data')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "- Our curve shows shape close to the top-left corner which shows good classification; however, the non-symmetric shape shows the false positive rate increases faster than the true positive rate, showing overall bias in classifier to predict false positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threshold Testing:\n",
    "We look at accuracy variation over different thresholds, and use this to set the unbias threshold to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of thresholds\n",
    "Nt = 100\n",
    "\n",
    "# Initialise threshold, accuracy and rate arrays\n",
    "thresholds = np.linspace(0,1,Nt)\n",
    "accuracys, accuracys_eq = np.zeros(Nt), np.zeros(Nt)\n",
    "rate, rate_eq = np.zeros([Nt,2]), np.zeros([Nt,2])\n",
    "\n",
    "# Loop to test cooresponding accuracy for each threshold\n",
    "for i, thres in enumerate(thresholds):\n",
    "    \n",
    "    # Predictions arrays are given by changing 'score' to binary labels using threshold(below threshold gives a 0)\n",
    "    y_pred    = np.where( y_score < thres, 0, 1).astype(bool)\n",
    "    y_pred_eq = np.where( y_score_eq < thres, 0, 1).astype(bool)\n",
    "    \n",
    "    # Use sklearn import to calculate accuracy\n",
    "    test_acc    = metrics.accuracy_score(y_true, y_pred, normalize=True)\n",
    "    test_acc_eq = metrics.accuracy_score(y_true_eq, y_pred_eq, normalize=True)\n",
    "\n",
    "    # Generate confusion matrix to find true positive, false positive, true negative, false negative values\n",
    "    con = metrics.confusion_matrix(y_true, y_pred)\n",
    "    con_eq = metrics.confusion_matrix(y_true_eq, y_pred_eq)\n",
    "    \n",
    "    # Store correct and wrong predictions\n",
    "    rate[i][0], rate[i][1] = (con[0,0]+con[1,1]), (con[1,0]+con[0,1])\n",
    "    rate_eq[i][0], rate_eq[i][1] = (con_eq[0,0]+con_eq[1,1]), (con_eq[1,0]+con_eq[0,1])\n",
    "    \n",
    "    # Store accuracys\n",
    "    accuracys[i]    = test_acc\n",
    "    accuracys_eq[i] = test_acc_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting accuracy against threshold\n",
    "fig,ax=plt.subplots(1,1, figsize=(10,5))\n",
    "\n",
    "ax.plot(thresholds, accuracys, label = 'Unbalanced data')\n",
    "ax.plot(thresholds, accuracys_eq, '--', label = 'Balanced data' )\n",
    "\n",
    "# Setting axis and labels\n",
    "ax.set_xlabel('Prediction Threshold')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Plot of Accuray over prediction Threshold')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- For the unbalanced data this bias is evident in Fig. 10 as reducing the threshold, such that predictions all become false, increases accuracy as the overall unbalanced distribution has approximately 88% false. This is regardless of classifier performance and the reverse is seen when increasing threshold to one. Comparatively, for balanced data the accuracy for a threshold of both one and zero gives 50% giving a more accurate baseline for analysis. We can use the intersection of the two graphs to set an unbiases threshold for analysis of the meta data as increased accuracy beyond the intersection is only due to any bias in data set. Thus, the threshold used in further analysis is 0.5 which indicates good separation between classification; this is further supported by the shape which shows high accuracy over a wide threshold range implying correct classifications and separation over range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Testing Meta Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up dictionaries:\n",
    "Create dictionaries holding corresponding scores and corect labels for each meta data strata to easy analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise dictionaries\n",
    "event_labels_dic, event_scores_dic = {},{}\n",
    "event_labels_dic_eq, event_scores_dic_eq = {},{}\n",
    "# Loops through all data sets\n",
    "for i in tqdm(range(1,30)):\n",
    "    # Produce images to test on model\n",
    "    event        = np.array(data[i]['cvnmap'], dtype='uint8')    \n",
    "    event_inputs = event.reshape(( event.shape[0],2,100,80 )).transpose(0,2,3,1)\n",
    "    \n",
    "    # Produce corresponding labels\n",
    "    event_l      = np.array(data[i]['neutrino']['interaction'],dtype='uint8')\n",
    "    event_labels = np.where( event_l > 3, 0, 1).astype(bool)\n",
    "    \n",
    "    # Predict scores fof images produced, also creates classed equal scores and labels\n",
    "    event_scores = model.predict(event_inputs, batch_size = 200)\n",
    "    event_scores_eq , event_labels_eq = ClassEqualised(event_scores, event_labels)\n",
    "\n",
    "    # Add to labels and scores dictionary, using kwargs of the function to add energies in bins of 5eV\n",
    "    event_labels_dic = Dictionary(event_labels_dic, event_labels, i, nuenergy = 5, lepenergy = 5, energy = 5)\n",
    "    event_scores_dic = Dictionary(event_scores_dic, event_scores, i, nuenergy = 5, lepenergy = 5, energy = 5)\n",
    "    \n",
    "    # Add to equalised labels and scores dictionary, using kwargs of the function to add energies in bins of 5eV\n",
    "    event_labels_dic_eq = Dictionary(event_labels_dic_eq, event_labels_eq, i, nuenergy = 5, lepenergy = 5, energy = 5)\n",
    "    event_scores_dic_eq = Dictionary(event_scores_dic_eq, event_scores_eq, i, nuenergy = 5, lepenergy = 5, energy = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure interaction key contains all interaction types to make looping easier\n",
    "for i in range(17):\n",
    "    if i in event_labels_dic['interaction']:\n",
    "        None\n",
    "    else:\n",
    "        event_labels_dic['interaction'][i] = []\n",
    "        event_scores_dic['interaction'][i] = [] \n",
    "\n",
    "    if i in event_labels_dic_eq['interaction']:\n",
    "        None\n",
    "    else:\n",
    "        event_labels_dic_eq['interaction'][i] = []\n",
    "        event_scores_dic_eq['interaction'][i] = []        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction Accuracy Test:\n",
    " Testing betweenw interaction type:\n",
    "- QE: Clean event, normally just two tracks\n",
    "- RES: Something in the middle\n",
    "- DIS: Messy event potentially many tracks and showers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Interaction_acc(Labels_dic, Scores_dic, thres, int_type):\n",
    "    ''' Function to calculate accuracy of given interaction type.\n",
    "    \n",
    "    Inputs:\n",
    "    Labels_dic - Dictionary containing all labels in meta data strata\n",
    "    Scores_dic - Dictionary containing all scores in meta data strata\n",
    "    thres      - Threshold from which scores are turned assigned 0/1\n",
    "    int_type   - Interaction type, 0: QE, 1: DIS, 2:Res, 4:Others\n",
    "    \n",
    "    Return:\n",
    "    Accuracy- Accuracy of Interaction type\n",
    "    '''\n",
    "    \n",
    "    # List comprehension to extract data from dictionary for thee interaction type given\n",
    "    Labels    = np.array( [ value for i in range(3) for value in Labels_dic['interaction'][int_type + 4*i]] )\n",
    "    Scores    = np.array( [ value for i in range(3) for value in Scores_dic['interaction'][int_type + 4*i]] )\n",
    "    \n",
    "    # Use threshold to produce predictions and calculate accuracy\n",
    "    Prediction = np.where( Scores < thres, 0, 1).astype(bool)\n",
    "    Accuracy   = metrics.accuracy_score(Labels, Prediction, normalize=True)\n",
    "    \n",
    "    return Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNuElectronElastic = 12\n",
    "# kNC =13  \n",
    "# kCosmic =14      \n",
    "# kOther =15   \n",
    "# kNIntType=16 \n",
    "\n",
    "# Set threshold discussed above\n",
    "thres = 0.5\n",
    "\n",
    "# Calculate accuracy for each interaction type\n",
    "QE_acc     = Interaction_acc(event_labels_dic, event_scores_dic, thres, 0)\n",
    "Res_acc    = Interaction_acc(event_labels_dic, event_scores_dic, thres, 1)\n",
    "DIS_acc    = Interaction_acc(event_labels_dic, event_scores_dic, thres, 2)\n",
    "Other_acc  = Interaction_acc(event_labels_dic, event_scores_dic, thres, 3)\n",
    "\n",
    "# Store Accuracys\n",
    "Type_keys = np.array(['QE_acc', 'DIS_acc', 'Res_acc', 'Other_acc'])\n",
    "Type_acc  = np.array([QE_acc, DIS_acc, Res_acc, Other_acc])\n",
    "\n",
    "print('QE accuracy:',QE_acc)\n",
    "print('Res accuracy:',Res_acc)\n",
    "print('DIS accuracy:',DIS_acc)\n",
    "print('Other accuracy:',Other_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of interaction types accuracy\n",
    "fig, ax = plt.subplots(1,1, figsize=(20,10))\n",
    "ax.bar(Type_keys, Type_acc)\n",
    "ax.set_title('Accuracy of Interaction Type')\n",
    "ax.set_xlabel('Interaction Type')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- This is the expected result as the QE are clearer two track interaction thus are more easily distinguished compared to DIS which are messier potentially with many tracks and showers. RES events have an accuracy in between as their tracks are less degradation then DIS but still less clear than QE events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise threshold array\n",
    "Nt = 100\n",
    "thresholds = np.linspace(0,1,Nt)\n",
    "\n",
    "# Initialise accuracy arrays\n",
    "QE_acc_thres, DIS_acc_thres, Res_acc_thres, Other_acc_thres = np.zeros(Nt), np.zeros(Nt), np.zeros(Nt), np.zeros(Nt)\n",
    "QE_acc_thres_eq, DIS_acc_thres_eq, Res_acc_thres_eq, Other_acc_thres_eq = (np.zeros(Nt), np.zeros(Nt), \n",
    "                                                                           np.zeros(Nt), np.zeros(Nt))\n",
    "# Calculate accuracy for all threshold values\n",
    "for i, thres in enumerate(thresholds):\n",
    "    # Calculate class equalised accuracis\n",
    "    QE_acc_thres_eq[i]     = Interaction_acc(event_labels_dic_eq, event_scores_dic_eq, thres, 0)\n",
    "    DIS_acc_thres_eq[i]    = Interaction_acc(event_labels_dic_eq, event_scores_dic_eq, thres, 1)\n",
    "    Res_acc_thres_eq[i]    = Interaction_acc(event_labels_dic_eq, event_scores_dic_eq, thres, 2)\n",
    "    Other_acc_thres_eq[i]  = Interaction_acc(event_labels_dic_eq, event_scores_dic_eq, thres, 3)\n",
    "    \n",
    "    # Calculate class non-equalised accuracis\n",
    "    QE_acc_thres[i]     = Interaction_acc(event_labels_dic, event_scores_dic, thres, 0)\n",
    "    DIS_acc_thres[i]    = Interaction_acc(event_labels_dic, event_scores_dic, thres, 1)\n",
    "    Res_acc_thres[i]    = Interaction_acc(event_labels_dic, event_scores_dic, thres, 2)\n",
    "    Other_acc_thres[i]  = Interaction_acc(event_labels_dic, event_scores_dic, thres, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracies variation over threshold\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "ax[0].plot(thresholds, QE_acc_thres,          linewidth=0.75,  label = 'QE Interaction')\n",
    "ax[0].plot(thresholds, Res_acc_thres,   '--', linewidth=0.75,  label = 'Res Interaction')\n",
    "ax[0].plot(thresholds, DIS_acc_thres,    ':', linewidth=0.75,  label = 'DIS Interaction')\n",
    "ax[0].plot(thresholds, Other_acc_thres, '-.', linewidth=0.75,  label = 'Other Interaction')\n",
    "\n",
    "# Setting axis and labels\n",
    "ax[0].set_xlabel('Predictive Threshold')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_title('Plot of Interaction accuracy over  threshold for balanced data ')\n",
    "ax[0].legend()\n",
    "\n",
    "# Balanced data plot\n",
    "ax[1].plot(thresholds, QE_acc_thres_eq,          linewidth=1,  label = 'QE Interaction')\n",
    "ax[1].plot(thresholds, Res_acc_thres_eq,   '--', linewidth=1,  label = 'Res Interaction')\n",
    "ax[1].plot(thresholds, DIS_acc_thres_eq,    ':', linewidth=1,  label = 'DIS Interaction')\n",
    "ax[1].plot(thresholds, Other_acc_thres_eq, '-.', linewidth=1,  label = 'Other Interaction')\n",
    "\n",
    "# Setting axis and labels\n",
    "ax[1].set_xlabel('Predictive Threshold')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].set_title('Plot of Interaction accuracy over threshold for unbalanced data ')\n",
    "ax[1].legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "- To ensure this behaviour is not a feature of the threshold chosen, the accuracy for the interactions is then plotted over a range of frequencies. Graph shows same characteristic shape of the unbalanced data , however, here the interaction type is split and show same ordering as above over the whole range, thus the correlation holds over the whole range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for testing other Meta Data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Meta_Accuracy(meta_data, Labels_dic, Scores_dic, thres):\n",
    "    ''' Function to calculate accuracy over the sub strata of given meta data type.\n",
    "    \n",
    "    Inputs:\n",
    "    Meta_data  - String of meta data; Accuracy calculated for over substrata of that meta data\n",
    "    Labels_dic - Dictionary contain Labels in meta data strata\n",
    "    Scores_dic - Dictionary contain Scores in meta data strata\n",
    "    thres      - Threshold in which Scores are converted to predictions\n",
    "    \n",
    "    Return:\n",
    "    meta_data_keys - Array containing meta data keys\n",
    "    meta_acc       - Array containing accuracies for each meta data key\n",
    "    '''\n",
    "    # Extract keys of meta data and sort\n",
    "    meta_data_keys = np.array( list(Labels_dic[meta_data].keys() ))\n",
    "    meta_data_keys.sort()\n",
    "\n",
    "    # Initialise array to hold accuracies\n",
    "    meta_acc = np.zeros(len(meta_data_keys))\n",
    "    \n",
    "    # For each meta data key calculate the accuracy\n",
    "    for i,v in enumerate(meta_data_keys):\n",
    "        \n",
    "        # Retrieve Labels and Scores corresponding for meta data key\n",
    "        Labels = np.array( Labels_dic[meta_data][v] )\n",
    "        Scores = np.array( Scores_dic[meta_data][v] )\n",
    "        \n",
    "        # Turns score to prediction and calculate accuracy\n",
    "        Prediction = np.where( Scores < thres, 0, 1).astype(bool)\n",
    "        Accuracy   = metrics.accuracy_score(Labels, Prediction, normalize=True)\n",
    "        \n",
    "        # Store accuracy for key\n",
    "        meta_acc[i] = Accuracy       \n",
    "       \n",
    "    return meta_data_keys, meta_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy Accuracy Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy accuracies\n",
    "lepenergy_keys,  lepenergy_acc  = Meta_Accuracy('lepenergy',  event_labels_dic, event_scores_dic, 0.5)\n",
    "nuenergy_keys,   nuenergy_acc   = Meta_Accuracy('nuenergy', event_labels_dic, event_scores_dic, 0.5)\n",
    "energy_keys,     energy_acc     = Meta_Accuracy('energy',  event_labels_dic, event_scores_dic, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy accuracies in bins of 5eV \n",
    "lepenergy5_keys,  lepenergy5_acc  = Meta_Accuracy('lepenergy5',  event_labels_dic, event_scores_dic, 0.5)\n",
    "nuenergy5_keys,   nuenergy5_acc   = Meta_Accuracy('nuenergy5', event_labels_dic, event_scores_dic, 0.5)\n",
    "energy5_keys,     energy5_acc     = Meta_Accuracy('energy5',  event_labels_dic, event_scores_dic, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(data):\n",
    "    '''Function to linearly fit data'''\n",
    "    fit, cvm = np.polyfit(data[0],  data[1], 1, cov='scaled')\n",
    "    dfit = [np.sqrt(cvm[i,i]) for i in range(2)]  \n",
    "\n",
    "    x   = np.linspace(0,np.max(data[0]),100)\n",
    "    y   = fit[0]*x + fit[1]\n",
    "    err = dfit[0]*x + dfit[1]\n",
    "    \n",
    "    return x, y, err\n",
    "\n",
    "nu_x, nu_y, nu_err = fit((nuenergy5_keys,  nuenergy5_acc))\n",
    "lep_x, lep_y, lep_err = fit((lepenergy5_keys[:-1],  lepenergy5_acc[:-1]))\n",
    "energy_x, energy_y, energy_err = fit((energy5_keys[:-1],  energy5_acc[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot of energy accuracies\n",
    "fig, ax = plt.subplots(1,3, figsize=(25,5))\n",
    "\n",
    "ax[0].plot(energy5_keys[:-1], energy5_acc[:-1],'.')\n",
    "ax[0].plot(energy_x,  energy_y,'--')\n",
    "ax[0].errorbar(energy_x[::5], energy_y[::5], yerr=energy_err[::5], color='black', linestyle='None', capsize=2)\n",
    "\n",
    "# Setting axis and labels\n",
    "ax[0].set_xlabel('Energy, eV')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_title('Plot of Accuracy over Lepton + Netrino Energy')\n",
    "\n",
    "ax[1].plot(lepenergy5_keys[:-1],  lepenergy5_acc[:-1],'.')\n",
    "ax[1].plot(lep_x,  lep_y,'--')\n",
    "ax[1].errorbar(lep_x[::5], lep_y[::5], yerr=lep_err[::5], color='black', linestyle='None', capsize=2)\n",
    "\n",
    "# Setting axis and labels\n",
    "ax[1].set_xlabel('Energy,eV')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].set_title('Plot of Accuracy over Lepton Energy')\n",
    "\n",
    "ax[2].plot(nuenergy5_keys,   nuenergy5_acc,'.')\n",
    "ax[2].plot(nu_x,  nu_y,'-')\n",
    "ax[2].errorbar(nu_x[::5], nu_y[::5], yerr=nu_err[::5], color='black', linestyle='None', capsize=2)\n",
    "# Setting axis and labels\n",
    "\n",
    "ax[2].set_xlabel('Energy, eV')\n",
    "ax[2].set_ylabel('Accuracy')\n",
    "ax[2].set_title('Plot of Accuracy over Netrino Energy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- the higher the energy the clearer the tract as greater ‘exposure’ in the image and high energy particles are more penetrative therefore will have long and more distinct tracks, in addition, there is less overlap visual in differences interaction type at high energy. This allows for better feature extraction and correlation of underlying image topology to given interaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing other meta data accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other meta data accuracies\n",
    "finalstate_keys, finalstate_acc           = Meta_Accuracy('finalstate',      event_labels_dic, event_scores_dic, 0.5)\n",
    "finalstateprong_keys, finalstateprong_acc = Meta_Accuracy('finalstateprong', event_labels_dic, event_scores_dic, 0.5)\n",
    "parent_keys,          parent_acc          = Meta_Accuracy('parent',          event_labels_dic, event_scores_dic, 0.5)\n",
    "evt_keys,             evt_acc             = Meta_Accuracy('evt',             event_labels_dic, event_scores_dic, 0.5)\n",
    "particles_keys,       particles_acc       = Meta_Accuracy('particles',       event_labels_dic, event_scores_dic, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels for finals states\n",
    "FinalState_keys = np.array([FinalState(i).name for i in finalstate_keys])\n",
    "\n",
    "# Plot final state accuracies\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,10))\n",
    "\n",
    "# Plot accuracy bars\n",
    "ax.bar(FinalState_keys, finalstate_acc)\n",
    "ax.tick_params(labelrotation=90)\n",
    "\n",
    "# Set labels and titles \n",
    "ax.set_title('Accuracy of Final State Type')\n",
    "ax.set_xlabel('Interaction Type')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Average for NuMu final states',np.mean(finalstate_acc[:16]),'\\nAverage for none NuMu final states', np.mean(finalstate_acc[16:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- For mu neutrino final states have lower accuracies than that of the other final state; when averaged we find mu neutrinos final states have an average accuracy of 74.2% compared to the other final states which have average accuracy of 88.6%. This could be due to the ubiquity of mu neutrinos there is a large number and thus wide variety of interaction recorded, therefore, greater overlap with other interaction types which leads to incorrect categorisation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot final state accuracies\n",
    "fig, ax = plt.subplots(2, 2, figsize=(25,10))\n",
    "\n",
    "ax[0,0].bar(finalstateprong_keys, finalstateprong_acc)\n",
    "ax[0,0].set_title('Accuracy for Final State prong')\n",
    "ax[0,0].set_xlabel('Final State prong')\n",
    "ax[0,0].set_ylabel('Accuracy')\n",
    "\n",
    "ax[0,1].bar(parent_keys,  parent_acc)\n",
    "ax[0,1].set_title('Accuracy for Parent type')\n",
    "ax[0,1].set_xlabel('Parent')\n",
    "ax[0,1].set_ylabel('Accuracy')\n",
    "\n",
    "ax[1,0].bar(evt_keys,   evt_acc)\n",
    "ax[1,0].set_title('Accuracy for Event type')\n",
    "ax[1,0].set_xlabel('Event type')\n",
    "ax[1,0].set_ylabel('Accuracy')\n",
    "\n",
    "\n",
    "ax[1,1].bar(particles_keys,   particles_acc)\n",
    "ax[1,1].set_title('Accuracy for Particle type')\n",
    "ax[1,1].set_xlabel('Particle')\n",
    "ax[1,1].set_ylabel('Accuracy')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "- Analysing the other meta data shows no other dependency on accuracy as can be seen in the data in Fig.15. There is not trend in the data, however, this may require processing to highlight any trend, which could be done with further study."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
